%%	SECCION documentclass																									 %%	
%%---------------------------------------------------------------------------%%
\documentclass[a4paper]{report}
%%---------------------------------------------------------------------------%%
%%	SECCION usepackage																											 %%	
%%---------------------------------------------------------------------------%%
\usepackage{amsmath, amsthm}
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{caratula}
\usepackage{hyperref} %para que haya hiperv�nculos
\usepackage{graphicx} % Para el logo magico!
\usepackage{alltt} % que es este paquete???

\usepackage[spanish]{babel} 
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
%\usepackage[latin1]{inputenc}

\oddsidemargin 0cm
\headsep -1.4cm
\textwidth=16.5cm
\textheight=23cm
%\makeindex



%%---------------------------------------------------------------------------%%
%%	SECCION document	                                                       %%	
%%---------------------------------------------------------------------------%%
\begin{document}
	\setcounter{page}{0} %para que numere a la caratula como p�gina 0 y a las demas a partir de 1

%%---- Caratula -------------------------------------------------------------%%
\title{Trabajo Práctico}
\author{Matias, Rodrigo y Martin\thanks{This is for making an acknowledgement.}
\\Universidad de Buenos Aires, Argentina}
\date{14 April, 2009}

\materia{Teoria de lenguajes}
\titulo{Trabajo Práctico}
\subtitulo{Generador de analizadores sintácticos}
\fecha{18 de junio, 2009}

\integrante{Rodrigo Campos}{561/06}{rodrigo@sdfg.com.ar}
\integrante{Martín Fernandez}{539/06/}{bondi007@gmail.com}
\integrante{Matías Pérez}{002/05}{elmaildematiaz@gmail.com}
%\resumen{}

\maketitle

%\tableofcontents

\newpage

\clearpage

\section*{An\'alisis del lenguaje de especificaci\'on de gram\'aticas}
Tal como planteaba el enunciado de este trabajo pr\'actico, el conjunto de producciones pertenecientes a la gram\'atica que definir\'a el lenguaje para el cual finalmente se generar\'a el 
analizador sint\'actico pertinente, se encontraba expresado en un lenguaje que especifica producciones de gram\'atica. Para dicho lenguaje se conoc\'ia, mediante el enunciado, su conjunto de producciones. As\'i, por ejemplo la cadena \textbf{ A:aB | C ; } escrita en el lenguaje mencionado, representaba la producci\'on A -> aB | C, para cualquier gram\'atica que la incluyera entre sus producciones.
Dada esta situaci\'on, se tornaba fundamental comprender qu\'e producci\'on estaba representando efectivamente cada cadena escrita en el mencionado lenguaje de especificaci\'on. Adem\'as de esto, era indispensable poder verificar si dada una cadena escrita con los s\'imbolos que define el lenguaje de especificaci\'on en cuesti\'on, la misma pertenece o no al lenguaje.
Ante estas necesidades, decidimos utilizar un analizador sint\'actico llamado \textbf{pyparsing}, para el lenguaje de programaci\'on \textbf{python}. Este analizador nos ofrec\'ia las funcionalidades requeridas y por ende nos permit\'ia verificar si dada una cadena, la misma pertenec\'ia a alg\'un lenguaje para el cual se le especificaran al mismo las reglas de construcci\'on de las producciones asociadas a su gram\'atica.
Sin embargo, en el momento de intentar utilizar pyparsing para analizar el lenguaje de especificaciones nos encontramos con que el conjunto de producciones dado en el enunciado que defin\'ia la gram\'atica de dicho lenguaje, conformaba una gram\'atica ambig\"'ua. Por ende, antes de proseguir con el desarrollo, nos encontramos ante la necesidad de desambig\"'uar la gram\'atica definida en el enunciado de este trabajo pr\'actico. Con lo cual pasamos de tener inicialmente el siguiente conjunto de producciones:

\begin{itemize}
	\item G  -> P G | P
	\item P  -> m ':'  PD ';'
	\item PD -> n | t | $\lambda$ |PD | PD PD | PD '*' | PD '+' | PD '?'  | '(' PD ')'
\end{itemize}

(en donde \textbf{t} representa a un caracter en min\'uscula, que a su vez representa un s\'imbolo terminal en la gram\'atica que se intenta definir. El comportamiento de \textbf{n} es an\'alogo, pero respecto a los no terminales de la gram\'atica a definir, para lo cual n es un caracter en may\'uscula).

a tener finalmente el conjunto de producciones desambig\"'uado que se presenta a continuaci\'on

\begin{itemize}
	\item G 		-> Prod (OtrasProd)*
	\item Prod 	-> n ':' ProdDer ';'
	
\end{itemize}

Una vez desambig\"'uada la gram\'atica, podiamos utilizar finalmente pyparsing para analizarla. Ahora, nos resultaba bastante sencillo determinar si una cadena representaba o no una producci\'on v\'alida construida en base a la gram\'atica del lenguaje de especificacion de gram\'aticas. Sin embargo, todav\'ia nos quedaba la ardua tarea de generar el grafo con el cual trabajar\'ian los algoritmos de c\'alculo de \textbf{anulables}, \textbf{primeros}, \textbf{siguientes}, y \textbf{s\'imbolos directrices}, que son explicados posteriormente. En un principio creimos que podr\'iamos realizar ambas tareas ("`parsear"' la gram\'atica inicial y armar el grafo correspondiente a la gram\'atica definida por la gram\'atica inicial) en forma secuencial. Es decir, primero utilizar pyparsing para verificar que cada producci\'n definida estuviera bien construida y perteneciera al lenguaje y luego en base a un algoritmo implementado por nosotros construir el grafo en cuesti\'on. Lamentablemente, el intento result\'o fallido, dado que la implementaci\'on de dicho algoritmo deb\'ia contemplar una gran cantidad de casos particulares, y resultaba ademas de "`endeble"', complicado de explicar y de entender y extremadamente complejo a la hora de realizar tares de test y posterior "`debug"'. Con lo cual, decidimos encarar la soluci\'on de este problema por otro camino. Tomamos la decisi\'on entonces de revisar la documentaci\'on de pyparsing con el objetivo de verificar si el mismo no nos pod\'ia "`dar una mano"' con la construcci\'on del grafo. Finalmente, descubrimos que pyparsing permit\'ia realizar llamados a funciones auxiliares durante el proceso de an\'alisis de una cadena. De esta manera, cada vez que encontrabamos un operador (como ser '*', '+', '?'), un conector (como ser '|', o implicitamente un conector de concatenaci\'on), un s\'imbolo no terminal, o uno terminal pod\'iamos llamar a la funci\'on auxiliar correspondiente a cada uno de ellos que se encargaba de, en cada caso, modificar o seguir construyendo (o ambas simultaneamente) el grafo representativo de la gram\'atica en cuesti\'on.

\subsection*{Breve rese\~{n}a acerca de pyparsing}
Pyparsing es un analizador sint\'actico que, como tal, permite enunciar un conjunto de reglas sobre las cuales se construyen las cadenas v\'alidas de un lenguaje. Estas reglas guardan gran similitud con las producciones que usualmente se utilizan para describir el comportamiento de una gram\'atica. Mientras que en el lenguaje natural de gram\'aticas el s\'imbolo \textbf{->} se utiliza para denotar la producci\'on derecha que genera un s\'imbolo no terminal, en pyparsing este s\'imbolo se representa con \textbf{=}, con lo cual una producci\'on de la forma \textbf{A -> aB}, en pyparsing se reescribir\'ia como \textbf{A = a + B} (en donde el s\'imbolo '+' representa la concatenaci\'on en el lenguaje usual de producciones). Por otra parte, el operador '|' se representa en pyparsing con el mismo s\'imbolo. Adem\'as, el valor $\lambda$ es representado con la sentencia \textbf{Empty()}. La funcionalidad del s\'imbolo '*', por otra parte, es representada por la funci\'on \textbf{ZeroOrMore( cadena )}, mientras que las acciones del s\'imbolo '+' son llevadas a cabo por la func\'on \textbf{OneOrMore()}. Por ser python un lenguaje de programaci\'on, no podemos enunciar la producci\'on \textbf{A = a + B} sin haber definido antes que es 'B'. Esto puede ocasionar un problema si el s\'imbolo inicial de la gram\'atica tiene en su producci\'on un no terminal todav\'ia no definido. Para solucionar esto, alcanza con reemplazar a 'B' en la producci\'on correspondiente, por alguna variable auxiliar, por ejemplo de nombre \textbf{otraB}, en donde otraB es una instancia de la clase \textbf{Forward}. Para la misma, en su inicializaci\'on se puede describir cual es el comportamiento que se espera que cumpla (en este caso, generar 'B').

Otras funciones que resultaron de utilidad a la hora de utilizar pyparsing son:

\begin{itemize}
	\item setParseAction( nombreDeFuncion ), que fue utilizada en el momento de la creaci\'on del grafo correspondiente a la gram\'atica para la cual se desea generar el analizador sint\'actico. As\'i, por ejemplo, OneOrMore(valor).setParseAction(esConcat) invocar\'a a la funci\'on esConcat cuando ocurrar que la cadena valor aparezca una o mas veces consecutivamente.
	\item Literal( cadena ), que permite definifir distintos 
s\'imbolos a utilizar durante el an\'alisis.
\end{itemize}

\section*{Comprobar que la gram\'atica sea ELL(1)}

Para asgurarnos que la gramática sea ELL(1), hacemos dos checkeos en dos
momentos distintos.


Antes de hacer cualquier checkeo, reducimos la gramática (es
decir, hacemos que todos los no-terminales sean alcanzables y activos). Luego,
por el teorema visto en clase que dice:
\begin{itemize}
\item Para toda gramatica G reducida (osea, una en las cuales todos lo
	no-terminales son alcanzables y activos), si G es independiente de contexto
	y LL(1), entonces G no es recursiva a izquierda.
\end{itemize}


Checkeamos si tiene recursión a izquierda, y si es así, concluimos que no es
ELL(1).


Es decir, suponemos que la gramática es ELL(1). Sabemos entonces que no
tiene recursión a izquierda. Si al checkear vemos que tiene recursión a
izquierda, entonces llegamos a un absurdo que vino de suponer que era ELL(1). Es
decir que podemos concluir que la gramática no es ELL(1).


Para poder aplicar el teorema anterior es necesario ver que la gramática es
independiente de contexto. Para ver esto, podemos ver cómo se reescribiría cada
símbolo que agregan las expresions regulares en producciones de una gramática
independiente de contexto y ver entonces que cada gramática cuyas producciones
son expresiones regulares se puede transformar en una gramática independiente de
contexto que genera el mismo lenguaje.

\begin{itemize}
\item Por cada expresion A -> B | C, se puede transformar en dos producciones: A -> B
y A -> C.
\item Por cada expresion A -> B*, se puede transformar en: A -> BA y A -> $\lambda$
\item Por cada expresion A -> B+, se puede transformar en: A -> BA' y A' -> B y A' -> $\lambda$
\item Por cada expresion A -> B?, se puede transformar en: A -> B y A -> $\lambda$
\end{itemize}


Es fácil ver que ambas expresiones tienen el mismo lenguaje.


Para transformar una expresión regular cualquiera, lo que se hace es por cada
'B*' que tenga, se puede introducir un nuevo no-terminal A, tal que A -> BA y A
-> $\lambda$, y reemplazar 'B*' en la expresión regular por A. Haciendo lo mismo para el
resto de los símbolos siguiendo las transformaciones explicadas arriba, se puede
ver que cualquier expresión regular se puede transformar en una gramática
independiente de contexto equivalente.


Se puede notar también, de la forma de transformar las expresiones regulares,
que si la expresión regular tenía recursión a izquierda, entonces luego de
transformarlas también tendrán.


Es decir, si A -> B* tiene recursion a izquierda, entonces es A -> A*, y las
producciones generadas seran A -> AA y A -> $\lambda$, es decir que hay recursión a
izquierda. Es fácil ver que para el resto de los casos sucede lo mismo


El segundo checkeo para ver que sea ELL(1), es el que vimos en clase, que lo
hacemos luego de calcular anulables, primeros, siguientes y los simbolos
directrices. Es decir, checkeamos que para cada '|', los simbolos directrices de
sus hijos sean disjuntos y para cada '+', '*', '?', checkeamos que los primeros
del nodo hijo sea disjunto con los siguientes del nodo y que ningun hijo sea
anulable.


\section*{Reducir la gramática}

	Para reducir la gramática, primero buscamos cuáles son los nodos útiles
del grafo.


	Para esto primero buscamos los nodos activos, recorremos el grafo partiendo desde el
simbolo distinguido, y por cada nodo, si es un terminal, $\lambda$, '*' o '?', lo
marcamos como activo (pues siempre producen algo, ese terminal o $\lambda$). A cada
no-terminal o '|', lo marcamos como activo si alguno de sus hijos es útil. Y al
'.' y '+' los marcamos como activo sólo si todos sus hijos son útiles. Esto lo
hacemos hasta que recorramos el grafo entero sin marcar ningún nodo nuevo como activo.


	De esta forma tenemos cuáles son los nodos activos del grafo. Luego, lo
que hacemos es sacar todos los links que tenga un nodo a un nodo inactivo,
partiendo desde el símbolo distinguido. Es fácil ver que esto es correcto, pues:


\begin{itemize}
\item Si el nodo es no-terminal o '|', entonces esos casos, que nunca producirían nada,
se pueden ignorar.

\item Si el nodo es un '*' o '+' y sus hijos no son activos,
entonces lo reemplazamos por $\lambda$ y sacamos el link a su hijo, ya que es lo
único que pueden producir 

\item Si, en cambio, es un '.' (representaci\'on utilizada para la operaci\'on de concatenaci\'on) o un '+', el nodo est\'a activo si todos sus hijos tienen la misma condici\'on, por lo que si tienen un hijo 
inactivo ellos también lo son, y por tanto,
el padre sacará el link hacia ellos. Si no llegamos al no-terminal partiendo
desde el simbolo distinguido, el no-terminal era inalcanzable, por lo que lo podemos ignorar.
\end{itemize}


	Luego de este procedimiento convertimos a los nodos inactivos en inalcanzables, 
ya que los "desligamos" de la componente conexa que contiene al simbolo distinguido. Una vez 
que tenemos esto lo único que hace falta es quitar los nodos inalcanzables, para esto simplemente 
primero los reconocemos recorriendo la componente conexa y a medida que lo hacemos generamos 
el conjunto de los nodos de la misma, para luego reemplazar al conjunto de nodos del grafo 
por este nuevo conjunto.


	Finalmente, después de aplicar estos dos procedimientos, el grafo se
corresponde al de una gramática reducida (pues todos sus s\'imbolos no-terminales son
activos y alcanzables)


\section*{Checkear la recursi\'on a izquierda}

	Para analizar la recursi\'on a izquierda, usamos una idea muy similar a la que
utilizamos para realizar el c\'alculo de "`primeros"'. Recorremos el grafo de la misma manera, las
diferencias son:


\begin{itemize}

\item Si el nodo actual es un terminal o $\lambda$, lo ignoramos.

\item Si es '|', '*', '?', '+' o un no-terminal, hacemos lo mismo que hace primeros y además, para cada hijo si es un no-terminal que no pertenece a rec\_iz del nodo actual, lo agregamos y macarmos que cambió algo en
esta iteración.

\item Si es '.', hacemos lo mismo que hace primeros para cada hijo, solo que además también si el hijo es un no-terminal lo agregamos a rec\_iz del
nodo actual.

\end{itemize}


	Como consecuencia de esto, al finalizar tendremos un diccionario, el cual dado un nodo nos
dice con qué no-terminales puede comenzar (por esto es que es muy similar a
primeros). Si algún nodo puede comenzar con él mismo, entonces tiene recursión a
izquierda. Si, por el contrario, ninguno de ellos puede comenzar con si mismo, entonces no se tiene
recursión a izquierda presente en la gram\'atica.


\section*{Simbolos directrices}

Para calcular los simbolos directrices, primero calculamos para cada nodo, los
anulables, los primeros y los siguientes.


Luego creamos un diccionario que tenga una entrada por cada nodo del grafo cuyo
valor sea los primeros del mismo, y si dicho nodo es anulable, le agregamos los
siguientes del nodo.


Para todos estos algoritmos utilizamos la misma forma de recorrer el grafo:
recorremos el grafo hasta que en una recorrida entera del grafo no hayamos
agregado nada a lo que estamos calculando (anulables, primeros o siguientes). Si
no cambio nada en una recorrida completa del grafo, es porque entonces ya hemos
calculado lo que queríamos calcular.

\subsection*{Anulables}

	En cada paso (o nodo) entonces, lo que hacemos es fijarnos si el nodo
actual está en el conjunto de nodos anulables y sino y cumple ciertos
requisitos, lo agregamos y marcamos que cambió algo en esta iteración.


	Los requisitos dependen del nodo actual, si es un terminal, nunca es
anulable, por lo que no lo agregamos. Si es '*', '?' o '$\lambda$', siempre es anulable
por lo que lo agregamos. Si es un no-terminal o un '|', es anulable si alguno de
sus hijos es anulable. Y si es un '.' o '+', es anulable si todos sus hijos son
anulables.


	Al terminar de recorrer el grafo, tenemos el conjunto de nodos
anulables.

\subsection*{Primeros}

	En cada paso, lo que hacemos es fijarnos cierta relacion entre los primeros
del nodo actual y sus hijos, si no la cumplen los agregamos y marcamos que
cambió algo en esta iteración.


	Si el nodo actual es un no terminal y el caracter no pertenece a los
primeros del nodo actual, lo agregamos. Si es un $\lambda$, lo ignoramos. Si es un
'|', '*', '?', '+' o un no-terminal, entonces nos fijamos que contenga a los
primeros de todos sus hijos. Si no los contiene, los agregamos y marcamos que
algo cambio en esta iteracion. Y si es '.', nos fijamos si contiene a los
primeros de su primer hijo (el de más a la izquierda), si no los contiene lo
agregamos y marcamos que cambió algo. Si el primer hijo es anulable, nos fijamos
que contenga a los primeros del segundo hijo, si no los contiene los agregamos y
marcamos que cambió algo en esta iteración. Y así con los sucesivos hijos hasta
que no tenga más o encontrar uno no anulable.


	Cuando hayamos recorrido el grafo entero sin cambiar nada, entonces
sabemos que todas las condiciones se cumplen, por lo que es fácil ver que se ha
calculado los primeros de cada nodo correctamente.


\subsection*{Siguientes}

	En cada paso, lo que hacemos es fijarnos cierta relacion entre los
siguientes del nodo actual y sus hijos, si no la cumplen los agregamos y marcamos que
cambió algo en esta iteración.


	Si el nodo actual es un termian o $\lambda$, lo ignoramos. Si es un '|',
'*', '?', '+' o un no-terminal, entonces nos fijamos que los siguientes de todos
sus hijos incluyan a los siguientes del nodo actual. Si no es así, los incluímos
y marcamos que algo cambió en esta iteración. Si es un '.' nos fijamos que los
siguientes del '.' sean tambien siguientes de su último hijo (el de más a la
derecha), si no es así los agregamos y marcamos que cambió. Si el último hijo es
anulable, entonces nos fijamos que los siguientes de '.' también sean siguientes
del nodo anterior al último y lo agregamos si no los incluye. Y lo mismo con el
anterior y así sucesivamente hasta llegar a uno que no es anulable. También,
para cada dos hijos 'x' e 'y', consecutivos ('x' más a la izquierda que 'y'),
nos fijamos que los siguientes de 'x' contengan a los primeros de 'y', si no es
así los agregamos. Y si 'y' es anulable, nos fijamos que los siguientes de 'x'
contenga a los a los siguientes de 'y'.


	Cuando hayamos recorrido el grafo entero sin cambiar nada, entonces
sabemos que todas las condiciones se cumplen, por lo que es fácil ver que se ha
calculado los siguientes de cada nodo correctamente.


\section*{Generación de código}


Para generar el código que parsee las cadenas del lenguaje de la gramática que se 
obtiene como entrada cumpliendo con los requisitos pedidos se decidió elegir el lenguaje C++. Para hacerlo se procede de la siguiente forma.


Por un lado se tienen dos archivos \emph{ParserEll1.cpp} y \emph{Utilitario.cpp} estos contienen un main general y estandar
que lo único que hace es leer la cadena de entrada, setear un par de variables y llamar a la funcion "parsear()" 
que será generada en el archivo \emph{codigoParser.cpp} que es el que se generará. El archivo \emph{Utilitario.cpp} 
es el que contiene la lógica del match, que se encarga de ver si la letra pasada para realizar el match es la misma del \emph{TC} 
(Token Corriente), en caso de serlo aumenta el \emph{TC} y en caso contrario tira el error correspondiente.


Por otro lado se genera el código de parseo especifico para cada gramática. Esto se realiza escribiendolo en el standard output, 
por lo que es necesario ejecutarlo de la siguiente manera: \emph{python programa > "ParserEll1.cpp"} para redirigir la salida al archivo \emph{ParserEll1.cpp}.


El programa lo que hace es primero obtener los diccionarios \emph{siguientes} y \emph{primeros}, y una vez que se obtienen se imprimen una serie de lineas que corresponden a ciertos include genericos de las librerias que usará. Y luego se imprime para cada nodo No-Terminal una función, que tiene como nombre \emph{Porc\_<Nombre Nodo>()} y para escribir el cuerpo de la misma se recorre el subgrafo que define el nodo llegando hasta las hojas o los nodos no terminales que se encuentren, para cada hijo se escribe:


\begin{itemize}
\item Si es un Terminal se hace un \emph{match(<NombreNodo>);}
\item Si es un No Terminal se hace un \emph{Porc\_<Nombre Nodo>()}
\item Si es un ``|'' se hace una serie de \emph{if} para cada uno de sus hijos preguntando si el \emph{TC} está en los simbolos directrices de ese nodo, y dentro de cada if se coloca el cuerpo de los hijos y se sigue. Y se termina con un \emph{else} para el caso de error en el que se encuentra un caracter inesperado.
\item Si es un ``.'' se escribe directamente el codigo de todos sus hijos en orden.
\item Si es un ``*'' se escribe un \emph{while} preguntando si en \emph{TC} se encuentra en los Primeros de ese nodo.
\item Si es un ``+'' se actua de manera análoga al ``*'' pero con un \emph{do while} en vez del \emph{while}.
\end{itemize}


De esta menera se escribe todo el codigo correspondiente a esta gramática.


Para compilar el código simplemente se debe compilar \emph{ParserEll1.cpp} con los archivos \emph{Utilitario.cpp} y \emph{codigoParser.cpp} en el mismo directorio.


Para ejecutarlo con la cadena \emph{cadena} se debe ejecutar (en Linux): \emph{nombreDelEjecutable  $<<<$  ``cadena''}.


\end{document}